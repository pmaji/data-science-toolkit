---
title: "k-Nearest Neighbours Tutorials 3"
author: "William Bell"
date: '2018-09-11'
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The k-nearest neighbours function I developed in a previous script was developed in order to illustrate the k-nearest neighbours algorithm.  In this script on the other hand, we will use the class library.  The class library offers multiple functions for the purposes of classification, using k-NN but with many functions for the purposes of doing cross-validation, etc.

Consider the R code for the function:

```{r, echo = FALSE}
library(class)
library(rbenchmark)
source("./materialsKNNForGM/kNNFunctions.R")

knn
```

We see near the end the function .C() used to call compiled C++ code, in this case the function VR_knn built into the package.  This is the real heavy lifter, doing the majority of the computationally intensive work.  This makes the function kind of blackbox-y like I've said, but compiled C++ code is dramatically faster than code in an interpreted language like R.

We will compare the relative speeds of the function I've shown in previous tutorials with the class library's version to show the degree of improvement.  I am going to use the exact same dataset as in tutorial 1.

```{r, echo = FALSE}

set.seed(12) ## Do this to make a reproducible simulated dataset

simulatedDataGroup1 <- data.frame(X = rnorm(100), Y = rnorm(100)) ## This is the "reds" who are a group of widgits displaying values one
## would expect to see from a bivariate regular normal (mu = 0, sd = 1) between two independent variables.
simulatedDataGroup2 <- data.frame(X = rnorm(100, mean = 2), Y = rnorm(100, mean = 2)) ## This is the "blues" who are a group of widgits
## with a similar distribution but ofset in both variables by 2 up and to the right.

combinedData <- data.frame(matrix(nrow = 200, ncol = 2))                                                                    
combinedData[1:100, ] <- simulatedDataGroup1
combinedData[101:200, ] <- simulatedDataGroup2 ## This is our complete simulated dataset
groupMembership <- c(rep("red", 100), rep("blue", 100)) ## ... and our record of their group membership.

```

For this, I will use the benchmark function from the rbenchmark package using k = 5.

```{r bench}
benchmark(kNN(classifiedData = combinedData, 
              classification = groupMembership, 
              unclassifiedPoint = c(1, 1), 
              p = 2, 
              k = 5), 
          knn(train = combinedData, 
              test = c(1, 1), 
              k = 5, 
              cl = groupMembership)
          )

benchmark(kNN(classifiedData = combinedData, 
              classification = groupMembership, 
              unclassifiedPoint = c(1, 1), 
              p = 2, 
              k = 11), 
          knn(train = combinedData, 
              test = c(1, 1), 
              k = 11, 
              cl = groupMembership)
          )
```

What we see is that the class package's function is three to six times faster for a relatively small dataset, a single training point, and a relatively small k on my computer.  If we had larger values for the other parameters we would expect larger jumps in those speed enhancements as well.  There are other advantages as well, since knn is tested well enough to have a great deal more overhead in place to catch potential errors (like test and training data of different dimensionality - mine would give an indexing error in response to that), more eyes have gone over the code for it, it has more options in certain capacities, and generally it is good practice to use established functions.  Mine also can only applied to one point at a time (though it would be very simple to apply it down test dataset).

The knn function in the class library makes one choice that makes life simpler but not necessarily in a good way: it chooses what distance measure you're using.  The knn function only does euclidean distances, unlike my implementation which is designed to do any p-norm.  However for many purposes this isn't a problem, since for instance in the geometric morphometrics dataset I worked on in the previous tutorial, it was natural to use l2/euclidean distances.

Finally, let's consider the object produced by knn.

```{r knn}

knn(train = combinedData, 
              test = c(1, 1), 
              k = 11, 
              cl = groupMembership)

```

It produces a factor vector which displays all possibilities as factors, and the actual possibility for each test point in the vector.  
